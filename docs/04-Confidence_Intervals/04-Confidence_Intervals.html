<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>STAT 227: Statistical Design and Analysis</title>
    <meta charset="utf-8" />
    <meta name="author" content="Anthony Scotina" />
    <script src="libs/header-attrs-2.6/header-attrs.js"></script>
    <link rel="stylesheet" href="my-theme.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# STAT 227: Statistical Design and Analysis
## Bootstrap Confidence Intervals
### Anthony Scotina

---






class: center, middle, frame

# Introduction

---

# Recap

If *sampling* of a sample of size `\(n\)` is done at **random**, then the resulting sample is *unbiased* and **representative** of the **population**. 
- Thus, the **sample statistic** from the representative sample represents a "good guess" of the (unknown) **population parameter**. 

Using the `bowl` data frame in the `moderndive` R package, we used the **sample proportion**, `\(\hat{p}\)`, to estimate the **population proportion**, `\(p\)`. 

Generally, we will use the *sample* to **infer** about the *population*. 

---

# In reality...

In *most cases*, we don't have the population values like we did with the `bowl` data frame, and we don't take many samples from the population. 

- We only have a single sample of data from a larger population!

While the **sample statistic** represents our single *best guess* at the (unknown) **population parameter**, we would also like to create a *range of plausible values* for the population parameter. 

- This range is called a **confidence interval**. 

--

How do we use a single sample to get some idea of how other samples might vary in terms of their statistic values?

- **Bootstrapping**

---

# Needed Packages 


```r
library(mosaic)
library(moderndive)
library(nycflights13)
library(tidyverse)
library(infer)
```

---

class: center, middle, frame

# Bootstrapping

---

# `pennies_sample`


```r
pennies_sample
```

```
# A tibble: 50 x 2
      ID  year
   &lt;int&gt; &lt;dbl&gt;
 1     1  2002
 2     2  1986
 3     3  2017
 4     4  1988
 5     5  2008
 6     6  1983
 7     7  2008
 8     8  1996
 9     9  2004
10    10  2000
# … with 40 more rows
```

--

**Question**: What is the *average year* on US pennies in 2019?

- Can we use the sample of 50 pennies to help answer this question?

---

# Exploratory Data Analysis: Data Visualization


```r
gf_histogram( ~ year, data = pennies_sample)
```

&lt;img src="04-Confidence_Intervals_files/figure-html/unnamed-chunk-4-1.png" width="50%" /&gt;

---

# Exploratory Data Analysis: Data Visualization

.pull-left[
![](04-Confidence_Intervals_files/figure-html/unnamed-chunk-5-1.png)&lt;!-- --&gt;
]

.pull-right[
Most of the ages are between 1980 and 2000, though there is a large spike in 2018. 

- If this sample is **representative** of the population of US pennies, we would expect the distribution of *all* pennies' year to have a similar shape. 
]

---

# Exploratory Data Analysis: Summary Statistics

Because we are interested in the **mean** year of *all* US pennies in 2019, let's calculate the **sample mean**, `\(\bar{x}\)`, of our 50 pennies using `mean()`:

```r
x_bar = mean( ~ year, data = pennies_sample)
```

--

Therefore, our **point estimate** is `\(\bar{x}=1995.44\)`. This represents our *best guess* at the **population mean** age of all US pennies, `\(\mu\)`. 

---

# The Point Estimate

The point estimate of `\(\bar{x}=1995.44\)` represents a single guess at `\(\mu\)`. But what if we'd like another guess?
- For example, using the `bowl` data frame, we took *many samples* and calculated *many sample proportions*. 
- Can we do that here?

--

**No**, because we only have a single sample and *no population data*!

- Luckily, **bootstrapping** allows us to use a *single sample* to generate many different samples that will act as our way of approximating a sampling distribution. 

---

# Sampling Variability

**Why are we interested in this?**

*Every* **sample statistic** has some variability. 
- Suppose you take a random sample of 50 Simmons University students and five are left-handed. 

--

If you take a *different* random sample of 50 Simmons University students, how many would you *expect* to be left-handed?
- Suppose three are left-handed. Is that surprising?
- Would 40 left-handed students out of 50 be surprising?

**Two ways to measure variability**:

1. Theory (Central Limit Theorem, etc.) -- AP/Introductory Statistics
2. **Simulation** (e.g., *bootstrapping*) -- **THIS CLASS**

---

# The Bootstrapping Process

Bootstrapping uses a process of sampling **with replacement** from our original sample to create new bootstrap samples of the **same size** as our original sample.

We can generate a *single bootstrap sample* by using the `rep_sample_n()` function from earlier:



```r
bootstrap_sample1 = 
  rep_sample_n(pennies_sample, size = 50, reps = 1, replace = TRUE)
```

--

- Notice that `size=50`. This *isn't an arbitrary number*. When bootstrapping, the `size` value will *always be the same as the original sample size*!

- We add a new argument to `rep_sample_n()`, `replace = TRUE`. This means that when a penny is selected for our **bootstrap sample**, it has the chance to be selected *again*. 

---

# Why bootstrap?

Ideally, you are able to take thousands of samples from the same population and obtain an estimate (close to) an *exact* population parameter, and know exactly how much each sample **varies**. 
- **Example**: Taking many samples of red and white balls from a bowl of 2,400 balls

--

But this is **impossible**!
- So we *improvise* and copy our sample many times to **simulate** a population. 

---

# Practice

Using your **bootstrap sample**, perform the following:
- Construct a histogram of `year`. 
- Calculate the sample mean of `year`. 

*Hint*: These are identical calculations to the ones we performed earlier, we are just using the `bootstrap_sample1` data frame here instead of `pennies_sample`. 

---

# Solution


```r
gf_histogram( ~ year, data = bootstrap_sample1)
```

&lt;img src="04-Confidence_Intervals_files/figure-html/unnamed-chunk-9-1.png" width="30%" /&gt;


```r
mean( ~ year, data = bootstrap_sample1)
```

```
[1] 1995.18
```

---

# The Bootstrapping Process

Suppose we have a **random sample** of `\(n=6\)` people. 

.center[
&lt;img src="original_sample.png" width="40%" /&gt;
]

---

# The Bootstrap Sample

**Bootstrap sample**: Sample *with replacement* from the original sample, using the *same sample size*. 

.center[
&lt;img src="bootstrap_sample.png" width="80%" /&gt;
]

---

# Taking 6 Bootstrap Samples

You are not limited to a single bootstrap sample. You will likely need *many bootstrap samples* to better understand the variation in pennies' ages between samples. 

Let's take 6 bootstrap samples (using `reps = 6` in `rep_sample_n()`), construct histograms, and generate sample means (**bootstrap statistics**):


```r
six_bootstrap_samples =
  rep_sample_n(pennies_sample, size = 50, reps = 6, replace = TRUE)
```

---

# Bootstrapping

When using the bootstrap, it might help to think of our original sample *as if* it were the population. 

- If the sample is *representative*, then the population might as well just be tons of copies of the original sample. 

--

**Example**:

Meet some "data":
.center[
&lt;img src="ac_beau.jpeg" width="80" /&gt;&lt;img src="ac_diva.jpeg" width="80" /&gt;&lt;img src="ac_rod.jpeg" width="79" /&gt;&lt;img src="ac_pango.jpeg" width="81" /&gt;&lt;img src="ac_goose.jpeg" width="80" /&gt;&lt;img src="ac_dora.jpeg" width="79" /&gt;
]

---

# How Bootstrapping Works

**One Sample** `\(\implies\)` *One Sample Statistic*

.center[
&lt;img src="bootstrap1.png" width="280" /&gt;
]

---

# How Bootstrapping Works

**One Sample** `\(\implies\)` **Bootstrap Sample** `\(\implies\)` *Bootstrap Statistic*

.center[
&lt;img src="bootstrap2.png" width="548" /&gt;
]

---

# How Bootstrapping Works

**One Sample** `\(\implies\)` **Bootstrap Samples** `\(\implies\)` *Bootstrap Statistics*

.center[
&lt;img src="bootstrap3.png" width="540" /&gt;
]

---

# How Bootstrapping Works

**One Sample** `\(\implies\)` **Many Bootstrap Samples** `\(\implies\)` *Many Bootstrap Statistics*

.center[
&lt;img src="bootstrap4.png" width="541" /&gt;
]

---

# Why Bootstrapping Works

If the sample is **representative**, the *population* might as well be *many copies of the sample*. 

.center[
&lt;img src="bootstrap5.png" width="502" /&gt;
]

---

# Why Bootstrapping Works

If the sample is **representative**, the *population* might as well be *many copies of the sample*. 

.center[
&lt;img src="bootstrap6.png" width="616" /&gt;
]

---

# Taking 6 Bootstrap Samples


```r
gf_histogram( ~ year, data = six_bootstrap_samples) + 
  facet_wrap( ~ replicate)
```

&lt;img src="04-Confidence_Intervals_files/figure-html/unnamed-chunk-21-1.png" width="50%" /&gt;

**Note**: We used the `facet_wrap( ~ replicate)` layer to create separate plots for each `replicate`. 

---

# Taking 6 Bootstrap Samples


```r
mean(year ~ replicate, data = six_bootstrap_samples)
```

```
      1       2       3       4       5       6 
1996.50 1997.38 1995.44 1991.44 1993.76 1995.98 
```

--

Do you think 6 bootstrap samples is enough?
- We *should* take many more samples, such as 1,000!

Luckily there is a package made specifically to help with this kind of thing: `infer`

---

class: center, middle, frame

# The `infer` Package for Statistical Inference

---

# `infer`

The `infer` package provides a useful resource in building **confidence intervals** and conducting **hypothesis tests** (more on those later). 

There are several *verb-named functions* that build in order. 

.center[
&lt;img src="infer_hex.png" width="30%" /&gt;
]

**But first...**

---

# The Pipe Operator `%&gt;%`

Say you want to perform a set of operations on the `flights` data frame:

1. Take `flights`, then...

--

2. `filter` for *only* Jet Blue flights (carrier code "B6"), then...

--

3. `group_by` departure airport (`origin`), then...

--

4. `summarize` the *average flight distance* (`distance`) by departure airport. 

---

# The Pipe Operator `%&gt;%`

Say you want to perform a set of operations on the `flights` data frame:

**Use the pipe operator!** (`%&gt;%`)


```r
flights %&gt;%
  filter(carrier == "B6") %&gt;%
  group_by(origin) %&gt;%
  summarize(mean.distance = mean(distance))
```

```
# A tibble: 3 x 2
  origin mean.distance
  &lt;chr&gt;          &lt;dbl&gt;
1 EWR             815.
2 JFK            1114.
3 LGA            1030.
```

(Take STAT 228 this spring for more on `filter()`, `group_by()`, and `summarize()`!)

---

# The Pipe Operator `%&gt;%`

You would read this sequence as:

1. Take `flights`, then...
2. Use this output as the input to the next function, `filter`, then...
3. Use this output as the input to the next function, `group_by`, then...
4. Use this output as the input to the next function, `summarize`.

--

**The starting value will always be the data frame!**

- The sequence of functions that follow will consist of some combination of **verb-named functions** that we will discuss throughout class today. 
- The result will the transformed/modified data frame that you want. For example, 
    - A data frame consisting of only the subset of rows in flights corresponding to Jet Blue flights.
    - A data frame consisting of average flight distance separated by origin. 

---

# Step 1: `specify()`

The `specify()` function is used primarily to choose which variables will be the focus of the statistical inference.
- This is done using the `response = ` argument:


```r
pennies_sample %&gt;%
  specify(response = year)
```

```
Response: year (numeric)
# A tibble: 50 x 1
    year
   &lt;dbl&gt;
 1  2002
 2  1986
 3  2017
 4  1988
 5  2008
 6  1983
 7  2008
 8  1996
 9  2004
10  2000
# … with 40 more rows
```

---

# Step 2: `generate()` replicates

After `specify()`-ing the main variable of interest, we use `generate()` to generate **bootstrap samples** (i.e., *replicates*) from the original sample. 
- Here, we can easily create 1,000 bootstrap samples:

```r
thousand_bootstrap_samples = pennies_sample %&gt;%
  specify(response = year) %&gt;%
  generate(reps = 1000, type = "bootstrap")
```

If you use `View(thousand_bootstrap_samples)`, you will see that there are 50,000 rows! 
- We took 1,000 bootstrap samples, each of size 50. 

---

# Step 3: `calculate()` summary statistics

Once we have 1,000 **bootstrap samples**, we need to calculate a **summary statistic** for each sample. 
- In this example, the *summary statistic* is the **mean**. 

```r
bootstrap_distribution = pennies_sample %&gt;% 
  specify(response = year) %&gt;% 
  generate(reps = 1000, type = "bootstrap") %&gt;% 
  calculate(stat = "mean")
```

--

This generates a data frame with 1,000 rows: each row containing the sample mean of the respective **bootstrap sample**. 
- This set of 1,000 sample means represents a **bootstrap distribution**. 

---

# Step 4: `visualize()` the results

.center[
&lt;img src="infer.png" width="515" /&gt;
]

---

# Step 4: `visualize()` the results


```r
visualize(bootstrap_distribution)
```

&lt;img src="04-Confidence_Intervals_files/figure-html/unnamed-chunk-29-1.png" width="50%" /&gt;

---

# Comparing `infer` with alternate R code


```r
# infer pipeline:
pennies_sample %&gt;%
  specify(response = year) %&gt;%
  generate(reps = 1000, type = "bootstrap") %&gt;%            
  calculate(stat = "mean") %&gt;%
  visualize()
```


```r
# Original workflow
bootstrap_sample = rep_sample_n(pennies_sample, size = 50, reps = 1000, replace = TRUE)
bootstrap_means = mean(year ~ replicate, data = bootstrap_sample)
gf_histogram( ~ bootstrap_means)
```

---

class: center, middle, frame

# Confidence Intervals

---

# Confidence Intervals

A **confidence interval** gives a range of *plausible values* for a population parameter. 
- Using *only* a **sample statistic** to estimate a parameter is like fishing in a murky lake with a spear, and using a confidence interval is like fishing with a net.

.center[
&lt;img src="fishing.png" width="50%" /&gt;
]

---

# Confidence Intervals

Confidence intervals depend on a specified **confidence level**, with...
- *higher* confidence levels corresponding to *wider* confidence intervals, and
- *lower* confidence levels corresponding to *narrower* confidence intervals. 

Common **confidence levels** include 90%, 95%, and 99%.

--

Using the **bootstrap distribution** (i.e., `bootstrap_distribution` from earlier), there are two ways we can obtain a confidence interval for the population mean age of US pennies:

1. The **percentile method**

2. The **standard error method**

---

# The Percentile Method

The only thing you need to do here is to use the `get_ci()` function from the `infer` package:


```r
percentile_ci = bootstrap_distribution %&gt;% 
  get_ci(level = 0.95, type = "percentile") 
# 0.95 and "percentile" are default values
percentile_ci
```

```
# A tibble: 1 x 2
  lower_ci upper_ci
     &lt;dbl&gt;    &lt;dbl&gt;
1    1991.    2000.
```

--

The **percentile method** gives us the 2.5th and the 97.5th *percentiles* of the bootstrap distribution. 

- Our range of plausible values for the mean year of US pennies in 2019 is between 1991 and 2000 years, with **95% confidence**. 

---

# Visualizing the CI

A cool thing you can do in R is to use the `visualize()` function to plot the *confidence interval* on top of the bootstrap distribution histogram. 
- Run the following:


```r
visualize(bootstrap_distribution) + 
  shade_ci(endpoints = percentile_ci, 
           color = "hotpink", fill = "chartreuse")
```

---

# The Standard Error Method

Our **bootstrap distribution** is close to symmetric and bell-shaped, or **Normally distributed**. 

If this is the case, we can use a shortcut formula to calculate the lower and upper endpoints of the confidence interval: 
`$$\bar{x}\pm (multiplier)\cdot SE$$`
where `\(\bar{x}\)` is the **sample statistic** (`x_bar`, obtained earlier), and `\(SE\)` stands for **standard error**, or the *standard deviation of the bootstrap distribution*.
- For a 95% confidence interval, the value of *multiplier* is **1.96**. 

---

# Why 1.96?

For any distribution that is **Normally-shaped**, roughly 95% of the values lie within *two standard deviations of the mean*.

- In the case of a bootstrap distribution, these standard deviations are also **standard errors**. 

Thus for a 95% confidence interval for the **mean**, 95% of values of the bootstrap distribution will lie within `\(\pm\)` 1.96 (or nearly 2) *standard errors* of `\(\bar{x}\)`. 

--

In a **Normal distribution**, roughly *68%* of the values lie within *one standard deviation of the mean*. 

- What would the *multiplier* be for a 68% confidence interval?


---

# The Standard Error Method

As with the **percentile method**, we use the `get_ci()` function here. But there are a few slight differences:

```r
standard_error_ci = bootstrap_distribution %&gt;% 
* get_ci(level = 0.95, type = "se", point_estimate = x_bar)
standard_error_ci
```

```
# A tibble: 1 x 2
  lower_ci upper_ci
     &lt;dbl&gt;    &lt;dbl&gt;
1    1991.    2000.
```

- We have to specify that we want a CI using the **standard error method** (using `type = "se"`). 

- We also have to provide the **point estimate**, which you should calculate beforehand. 

---

# The Standard Error Method

As with the **percentile method**, we use the `get_ci()` function here. But there are a few slight differences:

```r
standard_error_ci = bootstrap_distribution %&gt;% 
  get_ci(level = 0.95, type = "se", point_estimate = x_bar)
standard_error_ci
```

```
# A tibble: 1 x 2
  lower_ci upper_ci
     &lt;dbl&gt;    &lt;dbl&gt;
1    1991.    2000.
```

- Our range of plausible values for the mean year of US pennies in 2019 is between 1991 and 2000 years, with **95% confidence**. 
    - This is *identical* to the CI obtained using the **percentile method** (b/c the bootstrap distribution is **Normal**)!
    
---

class: center, middle, frame

# Interpreting the Confidence Interval

---

# Interpreting the Confidence Interval

We have done a lot of calculations in this module, but *what does it all mean*?

The purpose of confidence intervals is to provide a range of plausible values for an *unknown* parameter (in this case, the population mean `\(\mu\)`). 
- Using the **percentile method**, one such interval is `\([1991, 2000]\)`. 

--

Let's modify `pennies_sample` to **simulate** a population of US pennies.

```r
pennies_pop = pennies_sample %&gt;%
  sample_n(10000, replace = TRUE)

mean(pennies_pop$year)
```

```
[1] 1995.133
```

--

Thus, `\(\mu=1995\)` (approx.) which *falls in the confidence interval*. 

---

# Using a new `pennies_sample`

We can use the `infer` pipeline to re-do the entire set of exercises today, but using a *new* sample of pennies from the `pennies` data frame:



```r
bootstrap_distribution_new = pennies_pop %&gt;%
  rep_sample_n(size = 50, reps = 1) %&gt;%
  specify(response = year) %&gt;% 
  generate(reps = 1000) %&gt;% 
  calculate(stat = "mean")
bootstrap_distribution_new %&gt;%
  get_ci()
```

```
## # A tibble: 1 x 2
##   lower_ci upper_ci
##      &lt;dbl&gt;    &lt;dbl&gt;
## 1    1991.    1999.
```

This new confidence interval *also* contains `\(\mu\)` (1995). 

---

# Calculating Many CIs

We can repeat this process 100 times (don't worry about the code), and generate 100 95% confidence intervals:

.center[
&lt;img src="ci_sim.png" width="60%" /&gt;
]

.center[

]

---

# Calculating Many CIs

Using 95% as our **confidence level**, *approximately* 94 of the CIs contained the population mean `\(\mu=1995.133\)`, while 6 did not. 

**What does this mean?**

- The procedure we have used to generate confidence intervals is "*95% reliable*" in that we can expect it to include the true population parameter **approximately** 95% of the time *if the process is repeated*.

---

# Back to our example...

**What is a precise interpretation of a confidence interval?**

Recall our *original 95% confidence interval* using the **percentile method**: `\([1991, 2000]\)`. 

**Interpretation**: We are **95% confident** that the average year on US pennies in 2019 is between 1991 and 2000, using the **percentile method**. 

---

# Interpretating a CI

In general...

**Precise interpretation**: If we repeated our sampling procedure a large number of times, we expect about 95% of the resulting confidence intervals to capture the value of the population parameter.

**Short-hand interpretation**: We are 95% “confident” that a 95% confidence interval captures the value of the population parameter.

---

# Width of Confidence Interval

**Confidence level**

In order to be more confident in our best guess of a range of values, we need to widen the range of values.
- The higher the *confidence level*, the wider a confidence interval will be. 

--


```r
bootstrap_distribution %&gt;% 
  get_ci(level = 0.80, type = "percentile") 
```

```
## # A tibble: 1 x 2
##   lower_ci upper_ci
##      &lt;dbl&gt;    &lt;dbl&gt;
## 1    1993.    1998.
```

---

# Width of Confidence Interval

**Confidence level**

In order to be more confident in our best guess of a range of values, we need to widen the range of values.
- The higher the *confidence level*, the wider a confidence interval will be. 


```r
bootstrap_distribution %&gt;% 
  get_ci(level = 0.95, type = "percentile") 
```

```
## # A tibble: 1 x 2
##   lower_ci upper_ci
##      &lt;dbl&gt;    &lt;dbl&gt;
## 1    1991.    2000.
```

---

# Width of Confidence Interval

**Confidence level**

In order to be more confident in our best guess of a range of values, we need to widen the range of values.
- The higher the *confidence level*, the wider a confidence interval will be. 


```r
bootstrap_distribution %&gt;% 
  get_ci(level = 0.99, type = "percentile") 
```

```
## # A tibble: 1 x 2
##   lower_ci upper_ci
##      &lt;dbl&gt;    &lt;dbl&gt;
## 1    1990.    2001.
```

---

# Impact of Sample Size

In general, *larger sample sizes tend to produce narrower confidence intervals*. 
- As our sample size increases, our estimate gets more *precise*. 
    - Also, the **standard error decreases**. 
    
For example, a 95% confidence interval with `\(n=50\)` will be *narrower* than a 95% confidence interval with `\(n=25\)`. 

---

# Bootstrap vs. Traditional Methods

One advantage of the **bootstrap** over **traditional** methods is that you can construct a CI for *any* statistic, rather than only a **mean** or **proportion**. 

The `diamonds` dataset in the `tidyverse` package contains prices of almost 54,000 diamonds.


```r
View(diamonds)
```


---

# Practice

Using the `diamonds` dataset, calculate a 95% confidence interval for the average price of diamonds. 

--

*Solution*


```r
bootstrap_dist_diamonds = 
  diamonds %&gt;%
  specify(response = price) %&gt;%
  generate(reps = 1000, type = "bootstrap") %&gt;%
  calculate(stat = "mean")

visualize(bootstrap_dist_diamonds) # Check for bell shape

diamonds_ci = bootstrap_dist_diamonds %&gt;%
  get_ci(level = 0.95, type = "percentile")
diamonds_ci
```

```
# A tibble: 1 x 2
  lower_ci upper_ci
     &lt;dbl&gt;    &lt;dbl&gt;
1    3896.    3968.
```

---

# Practice

Because `price` is heavily right-skewed (construct a histogram to see!), calculate a 95% confidence interval for the *median* diamond price. 

[**Hint**: The *only thing* you need to change in the previous code is `stat = ...` in `calculate()`.]

--

*Solution*


```r
gf_histogram( ~ price, data = diamonds)

diamonds_ci_median = diamonds %&gt;%
  specify(response = price) %&gt;%
  generate(reps = 1000, type = "bootstrap") %&gt;%
  calculate(stat = "median") %&gt;%
  get_ci(level = 0.95, type = "percentile")
diamonds_ci_median
```

```
# A tibble: 1 x 2
  lower_ci upper_ci
     &lt;dbl&gt;    &lt;dbl&gt;
1    2375.     2440
```

---

class: center, middle

# Bootstrap vs. Sampling Distributions

---

# Comparing Bootstrap and Sampling Distributions

Recall that in the last module, we constructed a **sampling distribution** of the proportion of red balls in a bowl, using the `bowl` dataset. 

- This was done by taking 1,000 *replications* of size 100 (or 10, or 50) *without replacement* from the "population" of 2,400 objects in the bowl. 

The reason the **bootstrap** is so useful in application is that we will never actually take 1,000 unique samples from a population. 

- The **bootstrap distribution** serves as an *approximation* to the sampling distribution that we constructed before. 

--

Let's compare the two distributions and see how well they match!

---

# Sampling Distribution (from Module 3)




```r
virtual_sample_50 = rep_sample_n(bowl, size = 50, reps = 1000)
prop_red_50 = tally(color ~ replicate, format = "proportion", 
                    data = virtual_sample_50)["red", ]
gf_histogram( ~ prop_red_50)
```

&lt;img src="04-Confidence_Intervals_files/figure-html/unnamed-chunk-49-1.png" width="50%" /&gt;

---

# Bootstrap Distribution




```r
set.seed(12)
bowl_sample_1 = rep_sample_n(bowl, size = 50, reps = 1)
bootstrap_distribution = bowl_sample_1 %&gt;% 
  specify(response = color, success = "red") %&gt;% 
  generate(reps = 1000, type = "bootstrap") %&gt;% 
  calculate(stat = "prop")
gf_histogram( ~ stat, data = bootstrap_distribution)
```

&lt;img src="04-Confidence_Intervals_files/figure-html/unnamed-chunk-51-1.png" width="40%" /&gt;
            
---

# Comparison

.pull-left[
![](04-Confidence_Intervals_files/figure-html/unnamed-chunk-52-1.png)&lt;!-- --&gt;

(1,000 sample proportions)
]

.pull-right[
![](04-Confidence_Intervals_files/figure-html/unnamed-chunk-53-1.png)&lt;!-- --&gt;

(1,000 bootstrap sample proportions)
]

---

# What do we notice?

The **center** of the bootstrap distribution is around ~0.4-0.45. 
- This is because the bootstrap distribution is centered around the *sample proportion*, which is 0.4. 

The **center** of the sampling distribution is around ~0.35-0.40. 
- This is because the sampling distribution is centered around the *population proportion*, which is 0.375. 

--

However, the **spread** is roughly *similar*. 


```r
sd( ~ prop_red_50)
```

```
[1] 0.06954258
```

```r
sd( ~ stat, data = bootstrap_distribution)
```

```
[1] 0.06670388
```

---

# What do we notice?

The bootstrap distribution will likely *not have the same center* as the sampling distribution, because we are resampling from the same sample over and over again. 

- In other words, bootstrapping cannot improve the *quality* of an **estimate**.

However even if the bootstrap distribution might not have the same center as the sampling distribution, it will likely have *very similar shape and spread*. 

- In other words, bootstrapping will give you a *good estimate* of the **standard error**.

---

# Summary

The **bootstrap** is a powerful tool that is used to give an idea of how much a statistic *varies from sample to sample*. 

- The **bootstrap distribution** of a sample statistic ( `\(\bar{x}\)`, `\(\hat{p}\)`, median, difference in means, etc.) is an *approximation* of the **sampling distribution**, and it is centered around the sample statistic. 

We can use the bootstrap to calculate **confidence intervals**, which give a *range of plausible values* for the population parameter with some degree of confidence. 

- The **percentile method** gives the 2.5th and 97.5th *percentiles* of the bootstrap distribution. 

- The **standard error method** can also be used, but the variable must be *Normally distributed*. 

    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"countIncrementalSlides": false,
"highlightLines": true
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
